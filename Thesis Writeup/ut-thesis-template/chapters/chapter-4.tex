\chapter{Conclusions} \label{ch:conclusion}
Genetic Algorithms should probably be used to optimize rather than classify.  Without considerably more work developing a better algorithm to create an extensible generic universal genetic classifier, there's simply too much theoretical ground to make up.  GAs still do a good job optimizing, and can get the same or better performance out of them, which is very similar to the real evolution.  This project felt like forcing bacteria with primitive flagella to race against other bacteria without developed motor skills, and then being shocked as even without any other advantages the flagella-equipped microbes won every time.\\In the end, evolutionary algorithms are a swiss-army knife.  Sure, a GA would be able to solve Linear Programming problems better than a support vector machine, but that isn't even in the realm of a fair comparison.  GAs are good at what they are- optimizing a fitness function over time in a way that resembles evolution.  They can easily be overwhelmed with genomes of great length, which is where real evolution has a huge edge.  First, it is massively and embarrassingly parallel.  Second, developmental biology is great at branch and bound.  In other words, genomes that will have trouble producing viable solutions will themselves be less likely to exist because of many pass or fail tests along the way.  Oh, and it also has a billion years to come up with solutions.  If there were a way for GAs to take advantage of these things that would be extremely valuable, and that might be a place for further research.  Another place for future research might be in finding similarly generic tools which we could leverage, like, perhaps, neural nets to take a first pass at a dataset and then using a Hunter-like approach to distinguish results- though, if neural nets are already involved, there would need to be a good reason to invoke a Hunter when some other neural net would likely get better results, especially given their lackluster performance here.  One reason might be that despite their mediocre performance they are extremely transparent: a Hunter will distill its decision making into a few unambiguous rules, where a neural net is incapable of doing such a thing well.  This transparency was a major motivating factor for this approach to begin with.\\
The optimizers performed well, given relatively very few generations.  The next area of research for them is combining them and connecting their outputs back into another optimizer for a difficult or intractable dataset, combining them in novel ways to get the maximum signal from a dataset, perhaps even a meta-optimizer that will optimize an ensemble of disparate optimizers, themselves optimizing primitive classifiers.  That seems like a promising way of limiting the search length for each population of optimizers while still searching a much larger space, though more research would be needed to show that it is viable.\\
$\overline{A}$ has proved to be a flawed metric, as to some degree all metrics are. We would still argue that it is less flawed than anything simpler.  Until this result, our findings with $\overline{A}$ is that it was usually $\approxeq$ to accuracy.  Here, however, whether because of the skew of the datasets or some other confounding factor, $\overline{A}$ was often very different from raw accuracy, which is a place for either further research or a reason to find a new metric \cite{jurman} includes a few methods which might be valuable for multi-class confusion matrices.  We can calculate these metrics for the classifications we have already gathered, however future work might include using one or both of them as better metrics for scoring multi-class classifiers.