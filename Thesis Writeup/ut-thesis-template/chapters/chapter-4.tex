\chapter{Conclusions} \label{ch:conclusion}
Genetic Algorithms should probably be used to optimize rather than classify.  Without considerably more work developing a better algorithm to create an extensible generic universal genetic classifier, there's simply too much theoretical ground to make up.  GAs still do a good job optimizing, and can get the same or better performance out of them, which is very similar to the real evolution.  This project felt like forcing a speedboat to race against kittens piloting a bucket, and then being shocked as even without any other advantages the speedboat won every time.\\In the end, evolutionary algorithms are a swiss-army knife.  Certainly, a GA would be able to solve linear programming problems better than a support vector machine, but that isn't even in the realm of a fair comparison.  GAs are good at what they do, that is, optimizing a fitness function over time in a way that resembles evolution.  They can easily be overwhelmed with genomes of great length, which is one area where real evolution has a huge edge.  First, it is massively and embarrassingly parallel.  Second, developmental biology is great at branch and bound.  In other words, genomes that will have trouble producing viable solutions will themselves be less likely to exist because of many pass or fail tests along the way.  Oh, and it also has a billion years or more to come up with solutions.  If there were a way for GAs to take advantage of additional parallelism or checkpoints that might be a place for further research.  Another place might be in finding similarly generic tools which we could leverage, like, perhaps, neural nets to take a first pass at a dataset and then using a Hunter-like approach to distinguish results, though, if neural nets are already involved, there would need to be a good reason to invoke a Hunter when some other neural net would likely get better results, especially given their lackluster performance here.  One possible reason for using Hunters despite their mediocre performance: they are extremely transparent.  A Hunter will distill its decision making into unambiguous rules (even if it is a slew of them), where a neural net by its nature relies on numerous hidden calculations.  This transparency was a major motivating factor for this approach to begin with.\\
Some other direction that might be worth pursuing is finding a way to combine more theoretical underpinnings with a GA.  For instance, here the mechanics available to the Hunter were somewhat lackluster.  All they could do were make boolean decisions based on the absolute values of features.  That's simply not enough for modern datasets.  As mentioned, GAs are extremely good at leveraging tools given to them and combining them in interesting ways, so one might consider improving the quality of the toolbox a useful avenue for further research. This could be branching on any number of concepts, including statistical ones or neural network inspired, or perhaps abandoning ensemble voting altogether and coming to some conclusion more akin to maximum likelihood estimation.  An example of that might be building different PDFs for each class from an arbitrary number of Gaussian functions and evaluating based on the resulting metrics.\\
$\overline{A}$ has proved to be a flawed metric, as to some degree all metrics are. We would still argue that it is less flawed than anything simpler.  Until this result, our findings with $\overline{A}$ is that it was usually $\approxeq$ to accuracy.    Here, however, whether because of the skew of the datasets or some other confounding factor, $\overline{A}$ was often very different from raw accuracy, which is a place for either further research or a reason to find a new metric. In hindsight, and even from an \textit{a priori} standpoint, it seems obvious that if you only consider the diagonal of a confusion matrix you're losing useful information.  \cite{jurman_comparison_2012} includes a few methods which might be valuable for multi-class confusion matrices.     Matthew's Correlation Coefficient and Confusion Entropy seem promising, and implementing them as a fitness function would be fairly straightforward.   We calculate these metrics for the classifications we have already gathered, however future work might include using one or a weighted combination of both as better metrics for scoring multi-class classifiers.
\\ 
The optimizers performed well, given relatively very few generations.  The next area of research for them is combining them and connecting their outputs back into another optimizer for a difficult or intractable dataset, combining them in novel ways to get the maximum signal from a dataset, perhaps even a meta-optimizer that will optimize an ensemble of disparate optimizers, themselves optimizing primitive classifiers.  That seems like a promising way of limiting the search length for each population of optimizers while still searching a much larger space, though more research would be needed to show that it is viable.  For instance, the meta-optimizer could have a few bits for each subordinate optimizer, probably of different types.  The first section of the bits would encode two numbers, the first for how many optimizers to train and the second for how many features to include.   Then, the subordinate optimizers would operate under the constraint of including no more than the number of features, and would separately and quickly evolve an ensemble of different classifiers.  These classifiers in turn would provide their output to the meta-optimizer who would then optimize a classifier of its own using all the features and the classifications made by its ensemble of classifiers as a new table.  This might be feasible given the speed with which many of the classifiers were able to evolve, and the robustness of the classifiers generated in such a manner.\\
